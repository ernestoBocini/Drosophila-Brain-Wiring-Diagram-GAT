{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the GCN Classificator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_geometric.nn as gnn\n",
    "import torch_geometric.transforms\n",
    "import torch_geometric.utils\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "import torch_geometric as pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyG Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PyG graph from ../data/graphs/pg_graph.pkl\n",
    "\n",
    "with open('../data/graphs/pg_graph.pkl', 'rb') as f:\n",
    "    pg_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 1090673], super_class=[49339], class=[49339], hemilineage=[49339], x=[49339, 53], edge_attr=[1090673, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[49339, 53], edge_index=[2, 1090673], edge_attr=[1090673, 6], y=[49339], train_mask=[49339], val_mask=[49339], test_mask=[49339])\n",
      "Dataset: ConnectomeData():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 53\n",
      "Number of classes: 8\n",
      "Number of nodes: 49339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ernestobocini/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Optional\n",
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "\n",
    "class ConnectomeData(InMemoryDataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super().__init__('.', transform, None, None)\n",
    "\n",
    "        # Load your connectome data and create the Data object\n",
    "        x = pg_graph.x\n",
    "        edge_index = pg_graph.edge_index\n",
    "        edge_attr = pg_graph.edge_attr\n",
    "        y = pg_graph.super_class # for the moment only consider the super_class\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n",
    "        split = torch_geometric.transforms.RandomNodeSplit(\n",
    "            num_val=0.1, num_test=0.2, key=\"y\"\n",
    "        )\n",
    "\n",
    "        data = split(data)\n",
    "\n",
    "        self.data, self.slices = self.collate([data])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "dataset = ConnectomeData()\n",
    "\n",
    "print(dataset.data)\n",
    "data = dataset.data\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}') # note we are only considering 'class'\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[49339, 53], edge_index=[2, 1090673], edge_attr=[1090673, 6], y=[49339], train_mask=[49339], val_mask=[49339], test_mask=[49339])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the GCN architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class MyGCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(123)\n",
    "\n",
    "        self.conv1 = GCNConv(dataset.num_features, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.conv3 = GCNConv(64, 32)\n",
    "        self.classifier = Linear(32, dataset.num_classes)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index).relu()\n",
    "\n",
    "        out = self.classifier(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyGCN(\n",
      "  (conv1): GCNConv(53, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 32)\n",
      "  (classifier): Linear(in_features=32, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gcn = MyGCN().to(device)\n",
    "print(gcn)\n",
    "\n",
    "optimizer = optim.Adam(gcn.parameters(), lr=0.1)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, graph, optimizer, criterion, num_epochs, target, device = device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph)\n",
    "        loss = criterion(out[graph.train_mask], graph[target][graph.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = get_accuracy(model, graph, graph.train_mask, target)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch} | Train loss {loss.item():.4f} | Train accuracy {acc:.4f}\"\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_accuracy(model, graph, mask, target):\n",
    "    model.eval()\n",
    "    pred = model(graph).argmax(dim=1)\n",
    "    return (pred[mask] == graph[target][mask]).sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ernestobocini/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch_geometric/utils/loop.py:370: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1680607596266/work/aten/src/ATen/native/mps/operations/Indexing.mm:218.)\n",
      "  edge_index = torch.cat([edge_index[:, mask], loop_index], dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss 15696526336.0000 | Train accuracy 0.0020\n",
      "Epoch 10 | Train loss 232091088.0000 | Train accuracy 0.1098\n",
      "Epoch 20 | Train loss 1.3018 | Train accuracy 0.6529\n",
      "Epoch 30 | Train loss 1.1790 | Train accuracy 0.6529\n",
      "Epoch 40 | Train loss 1.1333 | Train accuracy 0.6529\n",
      "Epoch 50 | Train loss 1.1183 | Train accuracy 0.6529\n",
      "Epoch 60 | Train loss 1.1092 | Train accuracy 0.6529\n",
      "Epoch 70 | Train loss 1.1040 | Train accuracy 0.6529\n",
      "Epoch 80 | Train loss 1.1007 | Train accuracy 0.6529\n",
      "Epoch 90 | Train loss 1.0979 | Train accuracy 0.6529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyGCN(\n",
       "  (conv1): GCNConv(53, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (conv3): GCNConv(64, 32)\n",
       "  (classifier): Linear(in_features=32, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(gcn, dataset.data.to(device), optimizer, loss_fn, 100, \"y\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this remains constant because it starts predicting everything as the same class (1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the GCN on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1,  ..., 1, 1, 1], device='mps:0')\n",
      "tensor(0.6579, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# let's try to predict the class of the nodes in the test set\n",
    "gcn.eval()\n",
    "pred = gcn(dataset.data.to(device)).argmax(dim=1)\n",
    "print(pred[dataset.data.test_mask])\n",
    "\n",
    "# let's see how many nodes we got right \n",
    "print((pred[dataset.data.test_mask] == dataset.data.y[dataset.data.test_mask]).sum() / dataset.data.test_mask.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pred[dataset.data.test_mask].cpu().numpy()\n",
    "b = pred[dataset.data.train_mask].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a != 1), sum(b != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ernestobocini/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(69, device='mps:0')\n",
      "tensor(32264, device='mps:0')\n",
      "tensor(7800, device='mps:0')\n",
      "tensor(5383, device='mps:0')\n",
      "tensor(1974, device='mps:0')\n",
      "tensor(1269, device='mps:0')\n",
      "tensor(480, device='mps:0')\n",
      "tensor(100, device='mps:0')\n",
      "0.6539248870062223\n"
     ]
    }
   ],
   "source": [
    "print(sum(dataset.data.y == 0))\n",
    "print(sum(dataset.data.y == 1))\n",
    "print(sum(dataset.data.y == 2))\n",
    "print(sum(dataset.data.y == 3))\n",
    "print(sum(dataset.data.y == 4))\n",
    "print(sum(dataset.data.y == 5))\n",
    "print(sum(dataset.data.y == 6))\n",
    "print(sum(dataset.data.y == 7))\n",
    "\n",
    "print(32264/len(dataset.data.y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a different approach: Graph Attention Methods (GATs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGraphModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.graphconv1 = graphnn.GATv2Conv(input_size, hidden_size)\n",
    "        self.graphconv2 = graphnn.GATv2Conv(hidden_size, hidden_size)\n",
    "        self.graphconv3 = graphnn.GATv2Conv(hidden_size, hidden_size)\n",
    "        self.graphconv4 = graphnn.GATv2Conv(hidden_size, output_size)\n",
    "        \n",
    "        self.relu = nn.LeakyReLU()\n",
    "      \n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        ####### YOUR ANSWER #######\n",
    "        x = self.graphconv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.graphconv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.graphconv3(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.graphconv4(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 1, ..., 2, 5, 5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.numpy().argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3669,  1.0000,  0.2130],\n",
       "        [ 1.0000,  2.0537, -0.7717],\n",
       "        [-0.8292,  1.5968,  1.0000]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(3,3).normal_(mean=0,std=1)\n",
    "a[range(0,3),torch.tensor([1, 0, 2]) ] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############## TRAIN FUNCTION #######################\n",
    "#####################################################\n",
    "\n",
    "# import the f1-score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch_geometric.nn as graphnn\n",
    "\n",
    "def evaluate(model, criterion, device, data):\n",
    "\n",
    "    score_list_batch = []\n",
    "\n",
    "    output = model(data.x, data.edge_index)\n",
    "    loss_test = criterion(output[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    # creat prediction vector predict of shape (len(data.y),8)\n",
    "    # predict[i,j] = 1 if the model predicts that node i is of class j\n",
    "    # predict[i,j] = 0 otherwise\n",
    "    predict = torch.zeros(len(data.y),8)\n",
    "    predict[range(0,len(data.y)),output.argmax(1)] = 1\n",
    "    \n",
    "    # compute the f1-score\n",
    "    score = f1_score(data.y[data.train_mask].numpy(), predict[data.train_mask], average=\"micro\")\n",
    "    return score\n",
    "\n",
    "\n",
    "def train(model, criterion, device, optimizer, max_epochs, data):\n",
    "\n",
    "    epoch_list = []\n",
    "    scores_list = []\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # logits is the output of the model\n",
    "        logits = model(data.x, data.edge_index).to(device)\n",
    "        # compute the loss\n",
    "        loss = criterion(logits[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch , loss.item()))\n",
    "        \n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # evaluate the model on the validation set\n",
    "            # computes the f1-score (see next function)\n",
    "            score = evaluate(model, criterion, device, data)\n",
    "            print(\"F1-Score: {:.4f}\".format(score))\n",
    "            scores_list.append(score)\n",
    "            epoch_list.append(epoch)\n",
    "\n",
    "    return epoch_list, scores_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'index' tensor needs to be one-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m one_hot\n\u001b[0;32m----> 2\u001b[0m dataset\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m one_hot(dataset\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49my, num_classes\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch_geometric/utils/one_hot.py:31\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(index, num_classes, dtype)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Taskes a one-dimensional :obj:`index` tensor and returns a one-hot\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mencoded representation of it with shape :obj:`[*, num_classes]` that has\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mzeros everywhere except where the index of last dimension matches the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m    dtype (torch.dtype, optional): The :obj:`dtype` of the output tensor.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m\u001b[39m tensor needs to be one-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m num_classes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     num_classes \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: 'index' tensor needs to be one-dimensional"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import one_hot\n",
    "dataset.data.y = one_hot(dataset.data.y, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[49339, 53], edge_index=[2, 1090673], edge_attr=[1090673, 6], y=[49339, 8], train_mask=[49339], val_mask=[49339], test_mask=[49339])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ernestobocini/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 | Loss: 28842885120.0000\n",
      "Accuracy: 0.6534\n",
      "Epoch 00002 | Loss: 8636674048.0000\n",
      "Epoch 00003 | Loss: 1622594688.0000\n",
      "Epoch 00004 | Loss: 2275024128.0000\n",
      "Epoch 00005 | Loss: 2876649728.0000\n",
      "Epoch 00006 | Loss: 25421989888.0000\n",
      "Epoch 00007 | Loss: 28595822592.0000\n",
      "Epoch 00008 | Loss: 51094102016.0000\n",
      "Epoch 00009 | Loss: 32293806080.0000\n",
      "Epoch 00010 | Loss: 4376730624.0000\n",
      "Epoch 00011 | Loss: 3996340224.0000\n",
      "Accuracy: 0.1092\n",
      "Epoch 00012 | Loss: 3868876288.0000\n",
      "Epoch 00013 | Loss: 4366283776.0000\n",
      "Epoch 00014 | Loss: 4838290944.0000\n",
      "Epoch 00015 | Loss: 5311051264.0000\n",
      "Epoch 00016 | Loss: 5663306240.0000\n",
      "Epoch 00017 | Loss: 5900253184.0000\n",
      "Epoch 00018 | Loss: 5989053440.0000\n",
      "Epoch 00019 | Loss: 6077882880.0000\n",
      "Epoch 00020 | Loss: 19925850112.0000\n",
      "Epoch 00021 | Loss: 33167579136.0000\n",
      "Accuracy: 0.0014\n",
      "Epoch 00022 | Loss: 41463767040.0000\n",
      "Epoch 00023 | Loss: 44394823680.0000\n",
      "Epoch 00024 | Loss: 49299562496.0000\n",
      "Epoch 00025 | Loss: 65092026368.0000\n",
      "Epoch 00026 | Loss: 77869662208.0000\n",
      "Epoch 00027 | Loss: 96257761280.0000\n",
      "Epoch 00028 | Loss: 120039587840.0000\n",
      "Epoch 00029 | Loss: 151403511808.0000\n",
      "Epoch 00030 | Loss: 196874780672.0000\n",
      "Epoch 00031 | Loss: 232509276160.0000\n",
      "Accuracy: 0.0014\n",
      "Epoch 00032 | Loss: 287616270336.0000\n",
      "Epoch 00033 | Loss: 335319334912.0000\n",
      "Epoch 00034 | Loss: 397122568192.0000\n",
      "Epoch 00035 | Loss: 551003226112.0000\n",
      "Epoch 00036 | Loss: 813892173824.0000\n",
      "Epoch 00037 | Loss: 1021684482048.0000\n",
      "Epoch 00038 | Loss: 1384810676224.0000\n",
      "Epoch 00039 | Loss: 1624554733568.0000\n",
      "Epoch 00040 | Loss: 1947417313280.0000\n",
      "Epoch 00041 | Loss: 2533722947584.0000\n",
      "Accuracy: 0.0014\n",
      "Epoch 00042 | Loss: 2433487994880.0000\n",
      "Epoch 00043 | Loss: 2470868942848.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(basic_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.005\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39m### TRAIN THE MODEL\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m epoch_list, basic_model_scores \u001b[39m=\u001b[39m train(basic_model, criterion, device, optimizer, max_epochs, dataset\u001b[39m.\u001b[39;49mdata)\n",
      "Cell \u001b[0;32mIn[75], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, device, optimizer, max_epochs, data)\u001b[0m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m \u001b[39m# logits is the output of the model\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m logits \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m \u001b[39m# compute the loss\u001b[39;00m\n\u001b[1;32m     41\u001b[0m loss \u001b[39m=\u001b[39m criterion(logits[data\u001b[39m.\u001b[39mtrain_mask], data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mtrain_mask])\n",
      "File \u001b[0;32m~/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mBasicGraphModel.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraphconv1(x, edge_index)\n\u001b[1;32m     17\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m---> 18\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraphconv2(x, edge_index)\n\u001b[1;32m     19\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     20\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraphconv3(x, edge_index)\n",
      "File \u001b[0;32m~/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch_geometric/nn/conv/gatv2_conv.py:250\u001b[0m, in \u001b[0;36mGATv2Conv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe usage of \u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_attr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39madd_self_loops\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39msimultaneously is currently not yet supported for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39medge_index\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mSparseTensor\u001b[39m\u001b[39m'\u001b[39m\u001b[39m form\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[39m# propagate_type: (x: PairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49m(x_l, x_r), edge_attr\u001b[39m=\u001b[39;49medge_attr,\n\u001b[1;32m    251\u001b[0m                      size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    253\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha\n\u001b[1;32m    254\u001b[0m \u001b[39massert\u001b[39;00m alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/epfl-network-ml/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:486\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         aggr_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[1;32m    484\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate(out, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39maggr_kwargs)\n\u001b[0;32m--> 486\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aggregate_forward_hooks\u001b[39m.\u001b[39;49mvalues():\n\u001b[1;32m    487\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (aggr_kwargs, ), out)\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Max number of epochs\n",
    "max_epochs = 200\n",
    "\n",
    "### DEFINE THE MODEL\n",
    "basic_model = BasicGraphModel(  input_size = dataset.num_features, \n",
    "                                hidden_size = 256, \n",
    "                                output_size = dataset.num_classes).to(device)\n",
    "\n",
    "### DEFINE LOSS FUNCTION\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "### DEFINE OPTIMIZER\n",
    "optimizer = torch.optim.Adam(basic_model.parameters(), lr=0.005)\n",
    "\n",
    "### TRAIN THE MODEL\n",
    "epoch_list, basic_model_scores = train(basic_model, criterion, device, optimizer, max_epochs, dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_fcn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m### F1-SCORE ON TEST DATASET\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m score_test \u001b[39m=\u001b[39m evaluate(basic_model, loss_fcn, device, test_dataloader)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBasic Model : F1-Score on the test set: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(score_test))\n\u001b[1;32m      5\u001b[0m \u001b[39m### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_fcn' is not defined"
     ]
    }
   ],
   "source": [
    "### F1-SCORE ON TEST DATASET\n",
    "score_test = evaluate(basic_model, loss_fcn, device, test_dataloader)\n",
    "print(\"Basic Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
    "\n",
    "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
    "def plot_f1_score(epoch_list, scores) :\n",
    "    plt.figure(figsize=[10,5])\n",
    "    plt.plot(epoch_list, scores)\n",
    "    plt.title(\"Evolution of F1S-Score w.r.t epochs\")\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.show()\n",
    "    \n",
    "plot_f1_score(epoch_list, basic_model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epfl-network-ml",
   "language": "python",
   "name": "epfl-network-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
